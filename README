文件说明：
整个文件可分为两个版本，一个为无redis版，一个为redis版，建议使用有redis版本，功能较为齐全。每个版本都有各自的两个文件夹，一个为框架文件夹，里面维护了整个框架的核心代码，另一个为project文件夹，是实际应用时需要编辑的，里面维护了应用框架的基本例子。另外的文件是有setup.py生成的文件，如果你对框架文件夹中的内容进行了修改，可以使用python setup.py install进行框架的重安装。

下载前安装：
python3.5以上版本
redis（可选，有非redis版本）
python第三方库：
（requests
lxml
redis
aioredis
aiohttp）


用来做什么，怎么做：
1、
维护代理ip池：在redis_project_dir中，自定义proxies文件内容，可以模仿例子中的写法，紧跟着在setting中进行设置，即可实现爬取网站的ip并对ip的可用性进行测试及筛选。
2、运用框架写爬虫爬取网站：
模仿redis_project__dir文件夹格式，自定义spider,pipeline,middlwares,setting运行main文件即可爬取。
3、学习框架
框架运行路线：以下将对整个框架进行一个整体介绍，加上源码的阅读，相信你会对整个框架有一个比较深刻的理解。

框架的构造：
整个框架是建立在scrapy的模型基础上进行搭建的，其中整个最基本骨架构造为：
a、从spider出发的，spider根据用户提供的初始url将其封装为内部的Request类；b、engine驱动Request进入spider_middleware，可以在spider_middleware对Request属性进行添加及删除
c、engine驱动Request进入schedule，Request添加进队列中
d、engin驱动从队列中取出Request,将Request交给downloder_middleware中间键进行下载前预处理
e、engine进一步将Request交给downloder进行页面的访问及下载，将结果交给内置类Response进行封装
f、封装好的Response通过引擎交给downloder_middleware进行处理
g、处理好的Response交给spider的parse函数进行解析
h、解析出来的内容，可以分为两部分，一部分为新的url，可以封装为Request，重新交给引擎，从b步骤重新开始。另一部分为我么想要的数据，可以封装为内置类Item
I、引擎将Item发给pipeline进行数据的处理与入库，整个过程基本完成

其他构造：
1、去重：
schedule会根据Request的url，method，headers，data进行hash（使用的是sha1），得到一个指纹，根据指纹判断此Request是否已爬，未爬则进入redis队列，并将指纹加入指纹集中。
2、redis实现分布式与持久化
为了实现爬虫的分布式与持久化（即断点续爬），框架引入了redis。利用redis中的列表数据类型，与框架中的schedule对接，将Request以及生成的指纹集存放至redis，多台电脑同时爬取时，从同一个redis中判断Request的入队与出队，实现分布式。
同时因为将要爬取的Request保存在redis中，所以爬虫中断后可以重新开始，但是由于出队与下载完成，再到解析完成，实现循环，中间存在时间差，会导致部分Request丢失。这里采用的是备份队列，Request在出队后，立刻复制一份进入备份队列，当该Request完成解析循环后，才将Request从备份队列中释放。每次重爬前，都先对备份队列进行检查，如果不为空，则将队列中的内容倒入正式队列中，重新循环，即实现了无损持久化
3、日志
在框架utils文件夹中，实现了logger类，对整个logging的配置进行统一，所有输出统一使用此类，可实现顺序规范输出，并将日志记录至本地
4、代理ip池
setting中对ip池的基本配置进行维护，当进入ip池爬取与筛选模式时，只有proxy里的spider会进行爬取，普通类的spider将会冻结。
代理ip的网站html都相对简单及统一，所以spider中实现了一个proxyspider类，自定义spider时可继承该类，只需简单的配置即可爬取网站中的代理ip及端口号，并将其拼接后如redis中，此处使用redis中的有序集合类型，每个ip入库时都会给予初始分。
紧接着根据你给出的测试网站，框架会从ip池中随机挑选一个ip进行请求校验，当请求成功时，将分数调至MAX，失败时减一，当分数低于MIN时，将此ip从ip池中移除。
通过setting设置，一键设置普通爬虫是否使用ip池进行反爬虫。
5、多线程
使用multiprocessing中dummy模块的Pool池，与multiprocessing中的Pool相比的多进程相比，他其实多线程，但他的速度也相当快，并且好处是可以共享全局变量
6、异步协程
在ip池的筛选中，可以选择多线程，也可以使用异步协程，此处使用了asyncio，aiohttp，aioredis，保证了单个线程的高并发，实测效果相当可以
7、请求重试
设置重试次数，给与Request重试次数的属性，在downloder中，若请求失败或请求状态码不符合时，对该Request进行重新请求，并将request重试次数加1，当达到预设重试请求次数时，丢弃该请求。
8、解析方案
给Response类添加解析方法，包括xpath，re，json
更多新技能解锁中，未完待续。。。

写到最后：
请大家多加支持，共同进步~


